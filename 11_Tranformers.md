# Transformers
Theses are neural network architectures that rely heavily on self-attention mechanisms.
# Types
1. Encoder only
  - Great at understanding text (classification, embedding, etc.).
  - BERT (Bidirectional Encoder Representations from Transformers)
  - RoBERTa (Robustly optimized BERT)
  - DistilBERT (smaller and faster BERT)
  - ALBERT (lite BERT with parameter sharing)
2. Decoder only
3. Encoder- Decoder Transformers
# Huggingface
# pipelines
## Dataset
## Models
