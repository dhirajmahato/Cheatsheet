
# Transformers
Theses are neural network architectures that rely heavily on self-attention mechanisms.
# Types based on design architecture and purpose.
1. Encoder only
  - Great at understanding text (classification, embedding, etc.).
  - BERT (Bidirectional Encoder Representations from Transformers)
  - RoBERTa (Robustly optimized BERT)
  - DistilBERT (smaller and faster BERT)
  - ALBERT (lite BERT with parameter sharing)
2. Decoder only
  - Great for text generation (autogressive tasks).
  - GPT (Generative Pretrained Transformer) family: GPT-2, GPT-3, GPT-4, GPT-4.5
  - LLaMA (Meta’s series) – designed to be efficient
  - Mistral, Grok (by xAI), Falcon – decoder-style, optimized for performance
3. Encoder- Decoder Transformers
  - Suited for translation, summarization, question-answering.
  - T5 (Text-to-Text Transfer Transformer)
  - BART (Bidirectional and Auto-Regressive Transformer)
  - mT5, MBART (multilingual variants)
# Huggingface
# pipelines
## Dataset
## Models
